{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTS"
      ],
      "metadata": {
        "id": "Z050WIhoGd69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModel,\n",
        "    AutoConfig,\n",
        "    get_cosine_schedule_with_warmup\n",
        ")\n",
        "from torch.optim import AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "from tqdm.auto import tqdm\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1iy3nj2GFBjJ",
        "outputId": "c2b44e0a-327e-487a-c4b0-de5c70b78d9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.7.1+cu128\n",
            "CUDA available: True\n",
            "GPU: NVIDIA GeForce RTX 5090\n",
            "GPU Memory: 34.2 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CONFIGURATION"
      ],
      "metadata": {
        "id": "WpA8n0VtGgRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    # Data paths\n",
        "    RAW_CSV_PATH: str = 'training.1600000.processed.noemoticon.csv'\n",
        "    OUTPUT_DIR: str = './outputs'\n",
        "\n",
        "    # MODEL\n",
        "    MODEL_NAME: str = \"roberta-large\"\n",
        "\n",
        "    # Tokenization\n",
        "    MAX_LEN: int = 96\n",
        "\n",
        "    # TRAINING\n",
        "    BATCH_SIZE: int = 128  #\n",
        "    ACCUMULATION_STEPS: int = 2\n",
        "    LEARNING_RATE: float = 1e-5\n",
        "    WEIGHT_DECAY: float = 0.01\n",
        "    EPOCHS: int = 8\n",
        "    WARMUP_RATIO: float = 0.06\n",
        "    MAX_GRAD_NORM: float = 1.0\n",
        "\n",
        "    # Advanced training\n",
        "    USE_LLRD: bool = True\n",
        "    LLRD_FACTOR: float = 0.9\n",
        "\n",
        "    # Regularization\n",
        "    DROPOUT: float = 0.1\n",
        "    ATTENTION_DROPOUT: float = 0.1\n",
        "    HIDDEN_DROPOUT: float = 0.1\n",
        "\n",
        "    # Data augmentation\n",
        "    USE_AUGMENTATION: bool = True\n",
        "    AUG_PROBABILITY: float = 0.2\n",
        "\n",
        "    # Precision\n",
        "    USE_AMP: bool = True\n",
        "    AMP_DTYPE: str = \"bfloat16\"\n",
        "\n",
        "    # Early stopping\n",
        "    PATIENCE: int = 3\n",
        "\n",
        "    # Hardware\n",
        "    NUM_WORKERS: int = 0\n",
        "    PIN_MEMORY: bool = True\n",
        "\n",
        "# Create config\n",
        "config = Config()\n",
        "\n",
        "\n",
        "print(f\"\\n MODEL:\")\n",
        "print(f\"  Name: {config.MODEL_NAME}\")\n",
        "print(f\"  Max length: {config.MAX_LEN}\")\n",
        "\n",
        "print(f\"\\n TRAINING (MEMORY OPTIMIZED):\")\n",
        "print(f\"  Batch size: {config.BATCH_SIZE}\")\n",
        "print(f\"  Accumulation steps: {config.ACCUMULATION_STEPS}\")\n",
        "print(f\"  Effective batch: {config.BATCH_SIZE * config.ACCUMULATION_STEPS}\")\n",
        "print(f\"  Learning rate: {config.LEARNING_RATE}\")\n",
        "print(f\"  Epochs: {config.EPOCHS}\")\n",
        "\n",
        "print(f\"\\ OPTIMIZATION:\")\n",
        "print(f\"  Mixed precision: {config.USE_AMP} ({config.AMP_DTYPE})\")\n",
        "print(f\"  LLRD: {config.USE_LLRD}\")\n",
        "print(f\"  Dropout: {config.DROPOUT}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gLF9kOJFBgw",
        "outputId": "06cc9f71-8a57-49a0-f9b9-dba66dc339ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " MODEL:\n",
            "  Name: roberta-large\n",
            "  Max length: 96\n",
            "\n",
            " TRAINING (MEMORY OPTIMIZED):\n",
            "  Batch size: 128\n",
            "  Accumulation steps: 2\n",
            "  Effective batch: 256\n",
            "  Learning rate: 1e-05\n",
            "  Epochs: 8\n",
            "\\ OPTIMIZATION:\n",
            "  Mixed precision: True (bfloat16)\n",
            "  LLRD: True\n",
            "  Dropout: 0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU SETUP & OPTIMIZATION\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    # Enable TF32 for faster computation on Ampere+ GPUs\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "    # Enable cuDNN benchmark for faster training\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # Set memory allocation strategy\n",
        "    torch.cuda.set_per_process_memory_fraction(0.95)  # Use 95% of GPU\n",
        "\n",
        "\n",
        "    print(f\"✓ TF32 enabled\")\n",
        "    print(f\"✓ cuDNN benchmark enabled\")\n",
        "    print(f\"✓ Memory fraction: 95%\")\n",
        "    print(f\"✓ Device: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "else:\n",
        "    print(\"Warning: CUDA not available, using CPU\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ODf6CBRFBeR",
        "outputId": "5074bdb0-84fd-45bf-9a45-9884e36dd26e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ TF32 enabled\n",
            "✓ cuDNN benchmark enabled\n",
            "✓ Memory fraction: 95%\n",
            "✓ Device: NVIDIA GeForce RTX 5090\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD RAW DATA\n",
        "\n",
        "cols = ['polarity', 'id', 'date', 'query', 'user', 'text']\n",
        "raw_df = pd.read_csv(config.RAW_CSV_PATH, encoding='latin-1', header=None, names=cols)\n",
        "\n",
        "print(f\"\\n✓ Loaded {len(raw_df):,} tweets\")\n",
        "\n",
        "# Convert labels: 0 (negative) stays 0, 4 (positive) becomes 1\n",
        "raw_df['label'] = raw_df['polarity'].map({0: 0, 4: 1})\n",
        "\n",
        "print(f\"\\n Class distribution:\")\n",
        "print(raw_df['label'].value_counts().sort_index())\n",
        "\n",
        "# Keep only text and label\n",
        "df = raw_df[['text', 'label']].copy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_V-acLGGFBbx",
        "outputId": "5f4595e2-b3ed-47ff-d70b-95d380425a00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ Loaded 1,600,000 tweets\n",
            "\n",
            " Class distribution:\n",
            "label\n",
            "0    800000\n",
            "1    800000\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TEXT PREPROCESSING\n",
        "\n",
        "import re\n",
        "\n",
        "# Emoticon mappings\n",
        "EMOTICONS = {\n",
        "    ':)': '<HAPPY>', ':-)': '<HAPPY>', ':]': '<HAPPY>', ':D': '<HAPPY>',\n",
        "    ':(': '<SAD>', ':-(': '<SAD>', ':[': '<SAD>',\n",
        "    ':P': '<LAUGH>', ':-P': '<LAUGH>',\n",
        "    ';)': '<WINK>', ';-)': '<WINK>',\n",
        "    '<3': '<LOVE>',\n",
        "    \":'(\": '<CRYING>', ':,(': '<CRYING>',\n",
        "    ':@': '<ANGRY>',\n",
        "    ':/': '<SKEPTICAL>', ':-/': '<SKEPTICAL>',\n",
        "}\n",
        "\n",
        "def preprocess_tweet(text):\n",
        "    \"\"\"Preprocess tweet text for sentiment analysis\"\"\"\n",
        "\n",
        "    # Convert to lowercase first for URL/mention detection\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    # Extract emoticons BEFORE lowercasing (they're case-sensitive!)\n",
        "    original_text = text\n",
        "    for emoticon, token in EMOTICONS.items():\n",
        "        original_text = original_text.replace(emoticon, f' {token} ')\n",
        "\n",
        "    # Now work with the text\n",
        "    text = original_text.lower()\n",
        "\n",
        "    # Replace URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '<URL>', text)\n",
        "\n",
        "    # Replace mentions\n",
        "    text = re.sub(r'@\\w+', '<USER>', text)\n",
        "\n",
        "    # Replace hashtags (keep the word)\n",
        "    text = re.sub(r'#(\\w+)', r'<HASHTAG> \\1', text)\n",
        "\n",
        "    # Normalize repeated characters (looove -> love)\n",
        "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
        "\n",
        "    # Detect emphasis (ALL CAPS words)\n",
        "    words = text.split()\n",
        "    processed_words = []\n",
        "    for word in words:\n",
        "        if word.isupper() and len(word) > 2 and not word.startswith('<'):\n",
        "            processed_words.append('<SHOUT>')\n",
        "            processed_words.append(word.lower())\n",
        "        else:\n",
        "            processed_words.append(word)\n",
        "    text = ' '.join(processed_words)\n",
        "\n",
        "    # Clean up whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "WsF6EkkZFBZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PREPROCESS ALL TWEETS\n",
        "\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "# Apply preprocessing\n",
        "print(\"\\nApplying preprocessing...\")\n",
        "df['clean_text'] = df['text'].progress_apply(preprocess_tweet)\n",
        "\n",
        "print(f\"\\n✓ Preprocessing complete!\")\n",
        "\n",
        "# Show examples\n",
        "print(f\"\\n Examples:\")\n",
        "for i in range(3):\n",
        "    print(f\"\\n{i+1}. Original: {df['text'].iloc[i][:80]}...\")\n",
        "    print(f\"   Cleaned:  {df['clean_text'].iloc[i][:80]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3MvENMoFBWp",
        "outputId": "88cb1353-1d83-438c-e33d-9d92fc678226"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Applying preprocessing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1600000/1600000 [00:11<00:00, 144203.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ Preprocessing complete!\n",
            "\n",
            " Examples:\n",
            "\n",
            "1. Original: @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got D...\n",
            "   Cleaned:  <USER> http <skeptical> /twitpic.com/2y1zl - a<URL> that's a bummer. you shoulda...\n",
            "\n",
            "2. Original: is upset that he can't update his Facebook by texting it... and might cry as a r...\n",
            "   Cleaned:  is upset that he can't update his facebook by texting it.. and might cry as a re...\n",
            "\n",
            "3. Original: @Kenichan I dived many times for the ball. Managed to save 50%  The rest go out ...\n",
            "   Cleaned:  <USER> i dived many times for the ball. managed to save 50% the rest go out of b...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD TOKENIZER & ADD SPECIAL TOKENS\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME, use_fast=True)\n",
        "\n",
        "print(f\"\\n✓ Loaded: {config.MODEL_NAME}\")\n",
        "print(f\"  Base vocabulary: {len(tokenizer):,} tokens\")\n",
        "\n",
        "# Add special tokens\n",
        "special_tokens = [\n",
        "    '<HAPPY>', '<SAD>', '<LAUGH>', '<WINK>', '<LOVE>', '<CRYING>',\n",
        "    '<ANGRY>', '<SKEPTICAL>', '<USER>', '<HASHTAG>', '<URL>',\n",
        "    '<SHOUT>', '<EMPHASIS>', '<SARCASM>', '<QUESTION>'\n",
        "]\n",
        "\n",
        "special_tokens_dict = {'additional_special_tokens': special_tokens}\n",
        "num_added = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "\n",
        "print(f\"\\n✓ Added {num_added} special tokens\")\n",
        "print(f\"  New vocabulary: {len(tokenizer):,} tokens\")\n",
        "print(f\"  Special tokens: {special_tokens[:5]}...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfEEmz3zFBUJ",
        "outputId": "d1a005b7-ce06-4149-8c33-c57401737d78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ Loaded: roberta-large\n",
            "  Base vocabulary: 50,265 tokens\n",
            "\n",
            "✓ Added 15 special tokens\n",
            "  New vocabulary: 50,280 tokens\n",
            "  Special tokens: ['<HAPPY>', '<SAD>', '<LAUGH>', '<WINK>', '<LOVE>']...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DATASET CLASS\n",
        "\n",
        "import random\n",
        "\n",
        "class TweetDataset(Dataset):\n",
        "    \"\"\"Dataset for tweets with optional augmentation\"\"\"\n",
        "\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=160, augment=False, aug_probability=0.2):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.augment = augment\n",
        "        self.aug_probability = aug_probability\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def augment_text(self, text):\n",
        "        \"\"\"Simple augmentation: random word deletion\"\"\"\n",
        "        if not self.augment or random.random() > self.aug_probability:\n",
        "            return text\n",
        "\n",
        "        words = text.split()\n",
        "        if len(words) <= 3:\n",
        "            return text\n",
        "\n",
        "        # Preserve sentiment tokens\n",
        "        sentiment_indices = [i for i, w in enumerate(words) if w.startswith('<') and w.endswith('>')]\n",
        "        deletable_indices = [i for i in range(len(words)) if i not in sentiment_indices]\n",
        "\n",
        "        if len(deletable_indices) <= 2:\n",
        "            return text\n",
        "\n",
        "        # Delete 10-20% of words\n",
        "        num_to_delete = max(1, int(len(deletable_indices) * random.uniform(0.1, 0.2)))\n",
        "        indices_to_delete = set(random.sample(deletable_indices, num_to_delete))\n",
        "\n",
        "        augmented_words = [words[i] for i in range(len(words)) if i not in indices_to_delete]\n",
        "        return ' '.join(augmented_words)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.augment:\n",
        "            text = self.augment_text(text)\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].squeeze(0),\n",
        "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "KBe5yHtqFBRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CREATE DATASETS & DATALOADERS\n",
        "\n",
        "# Create datasets\n",
        "print(\"\\nCreating datasets...\")\n",
        "\n",
        "train_dataset = TweetDataset(\n",
        "    texts=train_df['clean_text'].values,\n",
        "    labels=train_df['label'].values,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=config.MAX_LEN,\n",
        "    augment=True,\n",
        "    aug_probability=config.AUG_PROBABILITY\n",
        ")\n",
        "\n",
        "val_dataset = TweetDataset(\n",
        "    texts=val_df['clean_text'].values,\n",
        "    labels=val_df['label'].values,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=config.MAX_LEN,\n",
        "    augment=False\n",
        ")\n",
        "\n",
        "test_dataset = TweetDataset(\n",
        "    texts=test_df['clean_text'].values,\n",
        "    labels=test_df['label'].values,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=config.MAX_LEN,\n",
        "    augment=False\n",
        ")\n",
        "\n",
        "print(f\"✓ Train dataset: {len(train_dataset):,} samples\")\n",
        "print(f\"✓ Val dataset: {len(val_dataset):,} samples\")\n",
        "print(f\"✓ Test dataset: {len(test_dataset):,} samples\")\n",
        "\n",
        "# Create DataLoaders\n",
        "print(\"\\nCreating DataLoaders...\")\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config.BATCH_SIZE,  # 128\n",
        "    shuffle=True,\n",
        "    num_workers=config.NUM_WORKERS,  # 0\n",
        "    pin_memory=config.PIN_MEMORY,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config.BATCH_SIZE * 2,  # 256\n",
        "    shuffle=False,\n",
        "    num_workers=config.NUM_WORKERS,\n",
        "    pin_memory=config.PIN_MEMORY\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=config.BATCH_SIZE * 2,  # 256\n",
        "    shuffle=False,\n",
        "    num_workers=config.NUM_WORKERS,\n",
        "    pin_memory=config.PIN_MEMORY\n",
        ")\n",
        "\n",
        "print(f\"\\n Train loader: {len(train_loader):,} batches\")\n",
        "print(f\" Val loader: {len(val_loader):,} batches\")\n",
        "print(f\" Test loader: {len(test_loader):,} batches\")\n",
        "\n",
        "print(f\"\\n Configuration:\")\n",
        "print(f\"  Batch size: {config.BATCH_SIZE}\")\n",
        "print(f\"  Val/Test batch: {config.BATCH_SIZE * 2}\")\n",
        "print(f\"  Num workers: {config.NUM_WORKERS}\")\n",
        "print(f\"  Augmentation: Training only\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeqrIR8eFBO5",
        "outputId": "bf46702d-2e4d-4c15-ac53-944fab38ff2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Creating datasets...\n",
            "✓ Train dataset: 1,280,000 samples\n",
            "✓ Val dataset: 160,000 samples\n",
            "✓ Test dataset: 160,000 samples\n",
            "\n",
            "Creating DataLoaders...\n",
            "\n",
            " Train loader: 10,000 batches\n",
            " Val loader: 625 batches\n",
            " Test loader: 625 batches\n",
            "\n",
            " Configuration:\n",
            "  Batch size: 128\n",
            "  Val/Test batch: 256\n",
            "  Num workers: 0\n",
            "  Augmentation: Training only\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL COMPONENTS\n",
        "\n",
        "class MultiSampleDropout(nn.Module):\n",
        "    \"\"\"Multi-sample dropout for regularization\"\"\"\n",
        "    def __init__(self, hidden_size, num_labels, dropout_rate=0.1, num_samples=5):\n",
        "        super().__init__()\n",
        "        self.num_samples = num_samples\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.classifiers = nn.ModuleList([\n",
        "            nn.Linear(hidden_size, num_labels) for _ in range(num_samples)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits_list = []\n",
        "        for classifier in self.classifiers:\n",
        "            dropped = self.dropout(x) if self.training else x\n",
        "            logits = classifier(dropped)\n",
        "            logits_list.append(logits)\n",
        "        return torch.stack(logits_list).mean(dim=0)\n",
        "\n",
        "\n",
        "class AttentionPooling(nn.Module):\n",
        "    \"\"\"Attention-based pooling\"\"\"\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_size, 1, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask):\n",
        "        scores = self.attention(hidden_states).squeeze(-1)\n",
        "        scores = scores.masked_fill(attention_mask == 0, -1e9)\n",
        "        weights = F.softmax(scores, dim=1).unsqueeze(-1)\n",
        "        pooled = (hidden_states * weights).sum(dim=1)\n",
        "        return pooled\n",
        "\n",
        "\n",
        "class OptimizedSentimentModel(nn.Module):\n",
        "    \"\"\"Sentiment classification model\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str, num_labels: int = 2, dropout: float = 0.1, use_multi_sample_dropout: bool = True):\n",
        "        super().__init__()\n",
        "\n",
        "        model_config = AutoConfig.from_pretrained(model_name)\n",
        "        model_config.hidden_dropout_prob = dropout\n",
        "        model_config.attention_probs_dropout_prob = dropout\n",
        "\n",
        "        self.transformer = AutoModel.from_pretrained(model_name, config=model_config)\n",
        "        self.transformer.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "        hidden_size = model_config.hidden_size\n",
        "        self.attention_pooling = AttentionPooling(hidden_size)\n",
        "\n",
        "        combined_size = hidden_size * 4\n",
        "\n",
        "        if use_multi_sample_dropout:\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(combined_size, hidden_size),\n",
        "                nn.LayerNorm(hidden_size),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(dropout),\n",
        "                MultiSampleDropout(hidden_size, num_labels, dropout_rate=dropout)\n",
        "            )\n",
        "        else:\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(combined_size, hidden_size),\n",
        "                nn.LayerNorm(hidden_size),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(dropout),\n",
        "                nn.Linear(hidden_size, num_labels)\n",
        "            )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
        "        hidden_states = outputs.last_hidden_state\n",
        "\n",
        "        # Multi-pooling\n",
        "        cls_output = hidden_states[:, 0, :]\n",
        "\n",
        "        mask_expanded = attention_mask.unsqueeze(-1).expand(hidden_states.size()).float()\n",
        "        sum_embeddings = torch.sum(hidden_states * mask_expanded, dim=1)\n",
        "        sum_mask = torch.clamp(mask_expanded.sum(dim=1), min=1e-9)\n",
        "        mean_output = sum_embeddings / sum_mask\n",
        "\n",
        "        hidden_states_masked = hidden_states.clone()\n",
        "        hidden_states_masked[mask_expanded == 0] = -1e9\n",
        "        max_output, _ = torch.max(hidden_states_masked, dim=1)\n",
        "\n",
        "        attn_output = self.attention_pooling(hidden_states, attention_mask)\n",
        "\n",
        "        pooled_output = torch.cat([cls_output, mean_output, max_output, attn_output], dim=-1)\n",
        "\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss = F.cross_entropy(logits, labels, weight=class_weights)\n",
        "\n",
        "        return {'loss': loss, 'logits': logits}\n",
        "\n"
      ],
      "metadata": {
        "id": "bGtZxQfhFBMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CREATE MODEL\n",
        "\n",
        "# Create model\n",
        "print(f\"\\nLoading: {config.MODEL_NAME}\")\n",
        "model = OptimizedSentimentModel(\n",
        "    model_name=config.MODEL_NAME,\n",
        "    num_labels=2,\n",
        "    dropout=config.DROPOUT,\n",
        "    use_multi_sample_dropout=True\n",
        ").to(device)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"\\nModel Statistics:\")\n",
        "print(f\"  Total parameters: {total_params / 1e6:.2f}M\")\n",
        "print(f\"  Trainable parameters: {trainable_params / 1e6:.2f}M\")\n",
        "print(f\"  Layers: {model.transformer.config.num_hidden_layers}\")\n",
        "print(f\"  Hidden size: {model.transformer.config.hidden_size}\")\n",
        "\n",
        "# Check GPU memory\n",
        "if device.type == 'cuda':\n",
        "    torch.cuda.empty_cache()\n",
        "    allocated = torch.cuda.memory_allocated() / 1e9\n",
        "    reserved = torch.cuda.memory_reserved() / 1e9\n",
        "    print(f\"\\n GPU Memory:\")\n",
        "    print(f\"  Allocated: {allocated:.2f} GB\")\n",
        "    print(f\"  Reserved: {reserved:.2f} GB\")\n",
        "    print(f\"  Available: {34 - reserved:.2f} GB\")\n",
        "\n",
        "print(\"\\n Model created successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02jvPgzYFBJ5",
        "outputId": "861cea52-d67d-4cc1-b4b6-519bd737a3a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading: roberta-large\n",
            "\n",
            "Model Statistics:\n",
            "  Total parameters: 360.63M\n",
            "  Trainable parameters: 360.63M\n",
            "  Layers: 24\n",
            "  Hidden size: 1024\n",
            "\n",
            " GPU Memory:\n",
            "  Allocated: 7.23 GB\n",
            "  Reserved: 7.96 GB\n",
            "  Available: 26.04 GB\n",
            "\n",
            " Model created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# OPTIMIZER & SCHEDULER\n",
        "\n",
        "def get_optimizer_grouped_parameters(model, learning_rate, weight_decay, llrd_factor=0.9):\n",
        "    \"\"\"Layer-wise learning rate decay\"\"\"\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\", \"LayerNorm.bias\"]\n",
        "    optimizer_grouped_parameters = []\n",
        "\n",
        "    # Classifier head\n",
        "    optimizer_grouped_parameters.append({\n",
        "        \"params\": [p for n, p in model.classifier.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "        \"weight_decay\": weight_decay,\n",
        "        \"lr\": learning_rate\n",
        "    })\n",
        "    optimizer_grouped_parameters.append({\n",
        "        \"params\": [p for n, p in model.classifier.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "        \"weight_decay\": 0.0,\n",
        "        \"lr\": learning_rate\n",
        "    })\n",
        "\n",
        "    # Attention pooling\n",
        "    optimizer_grouped_parameters.append({\n",
        "        \"params\": [p for n, p in model.attention_pooling.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "        \"weight_decay\": weight_decay,\n",
        "        \"lr\": learning_rate\n",
        "    })\n",
        "    optimizer_grouped_parameters.append({\n",
        "        \"params\": [p for n, p in model.attention_pooling.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "        \"weight_decay\": 0.0,\n",
        "        \"lr\": learning_rate\n",
        "    })\n",
        "\n",
        "    # Transformer layers (LLRD)\n",
        "    num_layers = model.transformer.config.num_hidden_layers\n",
        "    layers = [model.transformer.embeddings] + list(model.transformer.encoder.layer)\n",
        "    layers.reverse()\n",
        "\n",
        "    for i, layer in enumerate(layers):\n",
        "        layer_lr = learning_rate * (llrd_factor ** i)\n",
        "\n",
        "        optimizer_grouped_parameters.append({\n",
        "            \"params\": [p for n, p in layer.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": weight_decay,\n",
        "            \"lr\": layer_lr\n",
        "        })\n",
        "        optimizer_grouped_parameters.append({\n",
        "            \"params\": [p for n, p in layer.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "            \"weight_decay\": 0.0,\n",
        "            \"lr\": layer_lr\n",
        "        })\n",
        "\n",
        "    return optimizer_grouped_parameters\n",
        "\n",
        "# Get optimizer parameters\n",
        "if config.USE_LLRD:\n",
        "    print(f\"\\n✓ Using LLRD (factor: {config.LLRD_FACTOR})\")\n",
        "    optimizer_grouped_parameters = get_optimizer_grouped_parameters(\n",
        "        model, config.LEARNING_RATE, config.WEIGHT_DECAY, config.LLRD_FACTOR\n",
        "    )\n",
        "else:\n",
        "    optimizer_grouped_parameters = [\n",
        "        {\"params\": [p for n, p in model.named_parameters() if \"bias\" not in n and \"LayerNorm\" not in n],\n",
        "         \"weight_decay\": config.WEIGHT_DECAY},\n",
        "        {\"params\": [p for n, p in model.named_parameters() if \"bias\" in n or \"LayerNorm\" in n],\n",
        "         \"weight_decay\": 0.0}\n",
        "    ]\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=config.LEARNING_RATE, eps=1e-8, betas=(0.9, 0.999))\n",
        "\n",
        "# Calculate steps\n",
        "num_training_steps = len(train_loader) * config.EPOCHS\n",
        "num_warmup_steps = int(num_training_steps * config.WARMUP_RATIO)\n",
        "\n",
        "print(f\"\\n Training Schedule:\")\n",
        "print(f\"  Steps per epoch: {len(train_loader):,}\")\n",
        "print(f\"  Total steps: {num_training_steps:,}\")\n",
        "print(f\"  Warmup steps: {num_warmup_steps:,}\")\n",
        "\n",
        "# Create scheduler\n",
        "scheduler = get_cosine_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "# Create scaler\n",
        "if config.USE_AMP:\n",
        "    from torch.cuda.amp import GradScaler\n",
        "    scaler = GradScaler()\n",
        "    print(f\"\\n Mixed precision: {config.AMP_DTYPE}\")\n",
        "else:\n",
        "    scaler = None\n",
        "\n",
        "print(\"\\n Optimizer & scheduler ready!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuQN8MjxFBHR",
        "outputId": "a5a43602-dc7e-4833-a495-f3ccf86efae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ Using LLRD (factor: 0.9)\n",
            "\n",
            " Training Schedule:\n",
            "  Steps per epoch: 10,000\n",
            "  Total steps: 80,000\n",
            "  Warmup steps: 4,800\n",
            "\n",
            " Mixed precision: bfloat16\n",
            "\n",
            " Optimizer & scheduler ready!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING & EVALUATION FUNCTIONS\n",
        "\n",
        "def evaluate_model(model, dataloader, desc=\"Validation\"):\n",
        "    \"\"\"Evaluate model\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pbar = tqdm(dataloader, desc=desc, leave=False)\n",
        "        for batch in pbar:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            with torch.cuda.amp.autocast(enabled=config.USE_AMP, dtype=torch.bfloat16 if config.AMP_DTYPE == \"bfloat16\" else torch.float16):\n",
        "                outputs = model(input_ids, attention_mask, labels)\n",
        "\n",
        "            loss = outputs['loss']\n",
        "            logits = outputs['logits']\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary', zero_division=0)\n",
        "\n",
        "    return {'loss': avg_loss, 'accuracy': accuracy, 'precision': precision, 'recall': recall, 'f1': f1}\n",
        "\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, scheduler, scaler, epoch):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{config.EPOCHS}\")\n",
        "\n",
        "    for step, batch in enumerate(pbar):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=config.USE_AMP, dtype=torch.bfloat16 if config.AMP_DTYPE == \"bfloat16\" else torch.float16):\n",
        "            outputs = model(input_ids, attention_mask, labels)\n",
        "            loss = outputs['loss'] / config.ACCUMULATION_STEPS\n",
        "\n",
        "        if scaler is not None:\n",
        "            scaler.scale(loss).backward()\n",
        "        else:\n",
        "            loss.backward()\n",
        "\n",
        "        if (step + 1) % config.ACCUMULATION_STEPS == 0:\n",
        "            if scaler is not None:\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.MAX_GRAD_NORM)\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), config.MAX_GRAD_NORM)\n",
        "                optimizer.step()\n",
        "\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        total_loss += loss.item() * config.ACCUMULATION_STEPS\n",
        "\n",
        "        with torch.no_grad():\n",
        "            preds = torch.argmax(outputs['logits'], dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        pbar.set_postfix({'loss': f\"{total_loss / (step + 1):.4f}\", 'lr': f\"{scheduler.get_last_lr()[0]:.2e}\"})\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    train_accuracy = accuracy_score(all_labels, all_preds)\n",
        "    train_f1 = precision_recall_fscore_support(all_labels, all_preds, average='binary')[2]\n",
        "\n",
        "    return avg_loss, train_accuracy, train_f1\n"
      ],
      "metadata": {
        "id": "luTp4DmHFBEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MAIN TRAINING LOOP\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(config.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Training tracking\n",
        "best_f1 = 0.0\n",
        "best_epoch = 0\n",
        "best_model_state = None\n",
        "patience_counter = 0\n",
        "training_history = {\n",
        "    'train_loss': [], 'train_accuracy': [], 'train_f1': [],\n",
        "    'val_loss': [], 'val_accuracy': [], 'val_precision': [], 'val_recall': [], 'val_f1': [],\n",
        "    'learning_rates': []\n",
        "}\n",
        "\n",
        "print(f\"\\n Configuration:\")\n",
        "print(f\"  Model: RoBERTa-Large (360.95M params)\")\n",
        "print(f\"  Train samples: 1,280,000\")\n",
        "print(f\"  Batch size: {config.BATCH_SIZE}\")\n",
        "print(f\"  Effective batch: {config.BATCH_SIZE * config.ACCUMULATION_STEPS}\")\n",
        "print(f\"  Epochs: {config.EPOCHS}\")\n",
        "print(f\"\\n\" + \"=\"*80)\n",
        "\n",
        "start_time = datetime.now()\n",
        "\n",
        "for epoch in range(config.EPOCHS):\n",
        "    epoch_start = datetime.now()\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"EPOCH {epoch+1}/{config.EPOCHS}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    # Train\n",
        "    train_loss, train_acc, train_f1 = train_epoch(model, train_loader, optimizer, scheduler, scaler, epoch)\n",
        "\n",
        "    print(f\"\\nTraining Results:\")\n",
        "    print(f\"  Loss:     {train_loss:.4f}\")\n",
        "    print(f\"  Accuracy: {train_acc*100:.2f}%\")\n",
        "    print(f\"  F1 Score: {train_f1*100:.2f}%\")\n",
        "\n",
        "    # Validate\n",
        "    print(f\"\\n Validating...\")\n",
        "    val_metrics = evaluate_model(model, val_loader, desc=f\"Validation Epoch {epoch+1}\")\n",
        "\n",
        "    print(f\"\\nValidation Results:\")\n",
        "    print(f\"  Loss:      {val_metrics['loss']:.4f}\")\n",
        "    print(f\"  Accuracy:  {val_metrics['accuracy']*100:.2f}%\")\n",
        "    print(f\"  Precision: {val_metrics['precision']*100:.2f}%\")\n",
        "    print(f\"  Recall:    {val_metrics['recall']*100:.2f}%\")\n",
        "    print(f\"  F1 Score:  {val_metrics['f1']*100:.2f}%\")\n",
        "\n",
        "    # Save history\n",
        "    training_history['train_loss'].append(train_loss)\n",
        "    training_history['train_accuracy'].append(train_acc)\n",
        "    training_history['train_f1'].append(train_f1)\n",
        "    training_history['val_loss'].append(val_metrics['loss'])\n",
        "    training_history['val_accuracy'].append(val_metrics['accuracy'])\n",
        "    training_history['val_precision'].append(val_metrics['precision'])\n",
        "    training_history['val_recall'].append(val_metrics['recall'])\n",
        "    training_history['val_f1'].append(val_metrics['f1'])\n",
        "    training_history['learning_rates'].append(scheduler.get_last_lr()[0])\n",
        "\n",
        "    epoch_time = (datetime.now() - epoch_start).total_seconds() / 60\n",
        "    print(f\"\\n  Epoch time: {epoch_time:.1f} minutes\")\n",
        "\n",
        "    # Check if best model\n",
        "    if val_metrics['f1'] > best_f1:\n",
        "        best_f1 = val_metrics['f1']\n",
        "        best_epoch = epoch + 1\n",
        "        best_model_state = model.state_dict().copy()\n",
        "        patience_counter = 0\n",
        "\n",
        "        print(f\"\\n New best F1: {best_f1*100:.2f}%\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'best_f1': best_f1,\n",
        "            'val_metrics': val_metrics,\n",
        "            'config': config\n",
        "        }, f\"{config.OUTPUT_DIR}/best_model.pt\")\n",
        "        print(f\"✓ Checkpoint saved\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f\"\\n No improvement. Patience: {patience_counter}/{config.PATIENCE}\")\n",
        "        print(f\"   Best F1: {best_f1*100:.2f}% (Epoch {best_epoch})\")\n",
        "\n",
        "    # Early stopping\n",
        "    if patience_counter >= config.PATIENCE:\n",
        "        print(f\"\\n Early stopping triggered\")\n",
        "        break\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Time estimates\n",
        "    elapsed = (datetime.now() - start_time).total_seconds() / 60\n",
        "    avg_epoch_time = elapsed / (epoch + 1)\n",
        "    remaining = avg_epoch_time * (config.EPOCHS - (epoch + 1))\n",
        "\n",
        "    print(f\"\\n Elapsed: {elapsed:.1f} min | Estimated remaining: {remaining:.1f} min\")\n",
        "\n",
        "# Training complete\n",
        "training_time = (datetime.now() - start_time).total_seconds() / 60\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"TRAINING COMPLETE!\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"\\n  Total time: {training_time:.1f} minutes\")\n",
        "print(f\" Best validation F1: {best_f1*100:.2f}% (Epoch {best_epoch})\")\n",
        "\n",
        "# Load best model\n",
        "if best_model_state is not None:\n",
        "    model.load_state_dict(best_model_state)\n",
        "    print(f\"\\n✓ Loaded best model\")\n",
        "\n",
        "# Save history\n",
        "with open(f\"{config.OUTPUT_DIR}/training_history.json\", 'w') as f:\n",
        "    json.dump(training_history, f, indent=2)\n",
        "print(f\" Training history saved\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BuSfjpqFBCC",
        "outputId": "2c427921-27eb-43f1-9e85-065b74d116ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Configuration:\n",
            "  Model: RoBERTa-Large (360.95M params)\n",
            "  Train samples: 1,280,000\n",
            "  Batch size: 128\n",
            "  Effective batch: 256\n",
            "  Epochs: 8\n",
            "\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "EPOCH 1/8\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/8: 100%|██████████| 10000/10000 [37:42<00:00,  4.42it/s, loss=0.3746, lr=1.00e-05]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Results:\n",
            "  Loss:     0.3746\n",
            "  Accuracy: 82.64%\n",
            "  F1 Score: 82.41%\n",
            "\n",
            " Validating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation Results:\n",
            "  Loss:      0.2991\n",
            "  Accuracy:  87.20%\n",
            "  Precision: 87.96%\n",
            "  Recall:    86.20%\n",
            "  F1 Score:  87.07%\n",
            "\n",
            "  Epoch time: 39.3 minutes\n",
            "\n",
            " New best F1: 87.07%\n",
            "✓ Checkpoint saved\n",
            "\n",
            " Elapsed: 39.6 min | Estimated remaining: 277.3 min\n",
            "\n",
            "================================================================================\n",
            "EPOCH 2/8\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/8: 100%|██████████| 10000/10000 [36:58<00:00,  4.51it/s, loss=0.3082, lr=9.88e-06]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Results:\n",
            "  Loss:     0.3082\n",
            "  Accuracy: 86.70%\n",
            "  F1 Score: 86.65%\n",
            "\n",
            " Validating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation Results:\n",
            "  Loss:      0.2889\n",
            "  Accuracy:  87.79%\n",
            "  Precision: 89.06%\n",
            "  Recall:    86.16%\n",
            "  F1 Score:  87.58%\n",
            "\n",
            "  Epoch time: 38.6 minutes\n",
            "\n",
            " New best F1: 87.58%\n",
            "✓ Checkpoint saved\n",
            "\n",
            " Elapsed: 78.5 min | Estimated remaining: 235.6 min\n",
            "\n",
            "================================================================================\n",
            "EPOCH 3/8\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/8: 100%|██████████| 10000/10000 [38:08<00:00,  4.37it/s, loss=0.2899, lr=9.55e-06]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Results:\n",
            "  Loss:     0.2899\n",
            "  Accuracy: 87.58%\n",
            "  F1 Score: 87.53%\n",
            "\n",
            " Validating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation Results:\n",
            "  Loss:      0.2875\n",
            "  Accuracy:  88.01%\n",
            "  Precision: 89.38%\n",
            "  Recall:    86.28%\n",
            "  F1 Score:  87.80%\n",
            "\n",
            "  Epoch time: 39.7 minutes\n",
            "\n",
            " New best F1: 87.80%\n",
            "✓ Checkpoint saved\n",
            "\n",
            " Elapsed: 118.5 min | Estimated remaining: 197.5 min\n",
            "\n",
            "================================================================================\n",
            "EPOCH 4/8\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/8: 100%|██████████| 10000/10000 [36:16<00:00,  4.59it/s, loss=0.2765, lr=9.03e-06]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Results:\n",
            "  Loss:     0.2765\n",
            "  Accuracy: 88.23%\n",
            "  F1 Score: 88.18%\n",
            "\n",
            " Validating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation Results:\n",
            "  Loss:      0.2849\n",
            "  Accuracy:  88.14%\n",
            "  Precision: 88.80%\n",
            "  Recall:    87.28%\n",
            "  F1 Score:  88.03%\n",
            "\n",
            "  Epoch time: 37.7 minutes\n",
            "\n",
            " New best F1: 88.03%\n",
            "✓ Checkpoint saved\n",
            "\n",
            " Elapsed: 156.5 min | Estimated remaining: 156.5 min\n",
            "\n",
            "================================================================================\n",
            "EPOCH 5/8\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/8: 100%|██████████| 10000/10000 [35:12<00:00,  4.73it/s, loss=0.2640, lr=8.32e-06]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Results:\n",
            "  Loss:     0.2640\n",
            "  Accuracy: 88.83%\n",
            "  F1 Score: 88.78%\n",
            "\n",
            " Validating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation Results:\n",
            "  Loss:      0.2892\n",
            "  Accuracy:  88.19%\n",
            "  Precision: 89.59%\n",
            "  Recall:    86.43%\n",
            "  F1 Score:  87.98%\n",
            "\n",
            "  Epoch time: 36.6 minutes\n",
            "\n",
            " No improvement. Patience: 1/3\n",
            "   Best F1: 88.03% (Epoch 4)\n",
            "\n",
            " Elapsed: 193.2 min | Estimated remaining: 115.9 min\n",
            "\n",
            "================================================================================\n",
            "EPOCH 6/8\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/8: 100%|██████████| 10000/10000 [35:15<00:00,  4.73it/s, loss=0.2522, lr=7.48e-06]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Results:\n",
            "  Loss:     0.2522\n",
            "  Accuracy: 89.38%\n",
            "  F1 Score: 89.33%\n",
            "\n",
            " Validating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation Results:\n",
            "  Loss:      0.2918\n",
            "  Accuracy:  88.19%\n",
            "  Precision: 88.89%\n",
            "  Recall:    87.28%\n",
            "  F1 Score:  88.08%\n",
            "\n",
            "  Epoch time: 36.7 minutes\n",
            "\n",
            " New best F1: 88.08%\n",
            "✓ Checkpoint saved\n",
            "\n",
            " Elapsed: 230.2 min | Estimated remaining: 76.7 min\n",
            "\n",
            "================================================================================\n",
            "EPOCH 7/8\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/8: 100%|██████████| 10000/10000 [35:12<00:00,  4.73it/s, loss=0.2404, lr=6.52e-06]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Results:\n",
            "  Loss:     0.2404\n",
            "  Accuracy: 89.93%\n",
            "  F1 Score: 89.89%\n",
            "\n",
            " Validating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation Results:\n",
            "  Loss:      0.3102\n",
            "  Accuracy:  88.11%\n",
            "  Precision: 88.23%\n",
            "  Recall:    87.94%\n",
            "  F1 Score:  88.09%\n",
            "\n",
            "  Epoch time: 36.6 minutes\n",
            "\n",
            " New best F1: 88.09%\n",
            "✓ Checkpoint saved\n",
            "\n",
            " Elapsed: 267.2 min | Estimated remaining: 38.2 min\n",
            "\n",
            "================================================================================\n",
            "EPOCH 8/8\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/8: 100%|██████████| 10000/10000 [35:12<00:00,  4.73it/s, loss=0.2286, lr=5.50e-06]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Results:\n",
            "  Loss:     0.2286\n",
            "  Accuracy: 90.51%\n",
            "  F1 Score: 90.47%\n",
            "\n",
            " Validating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation Results:\n",
            "  Loss:      0.3041\n",
            "  Accuracy:  88.01%\n",
            "  Precision: 88.78%\n",
            "  Recall:    87.03%\n",
            "  F1 Score:  87.89%\n",
            "\n",
            "  Epoch time: 36.6 minutes\n",
            "\n",
            " No improvement. Patience: 1/3\n",
            "   Best F1: 88.09% (Epoch 7)\n",
            "\n",
            " Elapsed: 303.8 min | Estimated remaining: 0.0 min\n",
            "\n",
            "================================================================================\n",
            "TRAINING COMPLETE!\n",
            "================================================================================\n",
            "\n",
            "  Total time: 303.8 minutes\n",
            " Best validation F1: 88.09% (Epoch 7)\n",
            "\n",
            "✓ Loaded best model\n",
            " Training history saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check what we have in memory\n",
        "print(\"Checking available variables...\")\n",
        "print(f\"model type: {type(model) if 'model' in locals() else 'Not found'}\")\n",
        "print(f\"tokenizer type: {type(tokenizer) if 'tokenizer' in locals() else 'Not found'}\")\n",
        "print(f\"best_model_state exists: {'best_model_state' in locals()}\")\n",
        "print(f\"config exists: {'config' in locals()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCWj_hG4nY2d",
        "outputId": "3273c789-42a2-4d47-b23d-40672c2a6db5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking available variables...\n",
            "model type: <class 'collections.OrderedDict'>\n",
            "tokenizer type: <class 'transformers.models.roberta.tokenization_roberta_fast.RobertaTokenizerFast'>\n",
            "best_model_state exists: True\n",
            "config exists: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SAVE\n",
        "\n",
        "import torch\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Create timestamped directory to avoid overwriting\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "save_dir = f\"{config.OUTPUT_DIR}/final_model_{timestamp}\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Saving to: {save_dir}\")\n",
        "\n",
        "# 1. Save the model (handle both cases)\n",
        "try:\n",
        "    if isinstance(model, dict):\n",
        "        # model is already a state_dict\n",
        "        torch.save(model, os.path.join(save_dir, \"pytorch_model.bin\"))\n",
        "        print(\"Saved model state_dict\")\n",
        "    else:\n",
        "        # model is an object\n",
        "        torch.save(model.state_dict(), os.path.join(save_dir, \"pytorch_model.bin\"))\n",
        "        torch.save(model, os.path.join(save_dir, \"model_complete.pth\"))\n",
        "        print(\"Saved model weights and complete model\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving model: {e}\")\n",
        "\n",
        "# 2. Save best model state if available\n",
        "try:\n",
        "    if 'best_model_state' in locals() and best_model_state is not None:\n",
        "        torch.save(best_model_state, os.path.join(save_dir, \"best_model_state.bin\"))\n",
        "        print(\"Saved best model state\")\n",
        "except Exception as e:\n",
        "    print(f\"Note: Could not save best_model_state: {e}\")\n",
        "\n",
        "# 3. Save tokenizer\n",
        "try:\n",
        "    tokenizer.save_pretrained(save_dir)\n",
        "    print(\"✓ Saved tokenizer\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving tokenizer: {e}\")\n",
        "\n",
        "# 4. Save all training artifacts\n",
        "try:\n",
        "    # Save training history\n",
        "    if 'training_history' in locals():\n",
        "        with open(os.path.join(save_dir, \"training_history.json\"), 'w') as f:\n",
        "            json.dump(training_history, f, indent=2)\n",
        "        print(\"✓ Saved training history\")\n",
        "\n",
        "    # Save config\n",
        "    if 'config' in locals():\n",
        "        config_dict = {k: v for k, v in config.__dict__.items() if not k.startswith('__')}\n",
        "        with open(os.path.join(save_dir, \"training_config.json\"), 'w') as f:\n",
        "            json.dump(config_dict, f, indent=2)\n",
        "        print(\"✓ Saved training config\")\n",
        "\n",
        "    # Save metrics\n",
        "    metrics = {\n",
        "        'best_f1': best_f1 if 'best_f1' in locals() else None,\n",
        "        'best_epoch': best_epoch if 'best_epoch' in locals() else None,\n",
        "        'final_epoch': epoch + 1 if 'epoch' in locals() else None,\n",
        "    }\n",
        "    with open(os.path.join(save_dir, \"metrics.json\"), 'w') as f:\n",
        "        json.dump(metrics, f, indent=2)\n",
        "    print(\"✓ Saved metrics\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Warning: Some artifacts couldn't be saved: {e}\")\n",
        "\n",
        "# 5. Save the checkpoint file if it exists\n",
        "try:\n",
        "    checkpoint_path = f\"{config.OUTPUT_DIR}/best_model.pt\"\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        import shutil\n",
        "        shutil.copy(checkpoint_path, os.path.join(save_dir, \"checkpoint.pt\"))\n",
        "        print(\"✓ Copied checkpoint file\")\n",
        "except Exception as e:\n",
        "    print(f\"Note: Could not copy checkpoint: {e}\")\n",
        "\n",
        "# 6. Create a recovery script\n",
        "recovery_script = f'''# Recovery script generated on {datetime.now()}\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"{save_dir}\")\n",
        "\n",
        "# Load model state\n",
        "model_state = torch.load(\"{save_dir}/pytorch_model.bin\", map_location='cpu')\n",
        "\n",
        "# To use this, you'll need to recreate the model architecture:\n",
        "# model = OptimizedSentimentModel(...)\n",
        "# model.load_state_dict(model_state)\n",
        "\n",
        "print(\"Model state and tokenizer loaded successfully!\")\n",
        "'''\n",
        "\n",
        "with open(os.path.join(save_dir, \"recovery_script.py\"), 'w') as f:\n",
        "    f.write(recovery_script)\n",
        "\n",
        "\n",
        "print(f\"All files saved to: {save_dir}\")\n",
        "print(\"\\nSaved files:\")\n",
        "for file in os.listdir(save_dir):\n",
        "    print(f\"  - {file}\")\n",
        "\n",
        "# Also create a backup of the most important file\n",
        "print(\"\\nCreating backup of model weights...\")\n",
        "backup_dir = f\"{config.OUTPUT_DIR}/backup_{timestamp}\"\n",
        "os.makedirs(backup_dir, exist_ok=True)\n",
        "\n",
        "if isinstance(model, dict):\n",
        "    torch.save(model, os.path.join(backup_dir, \"model_backup.bin\"))\n",
        "else:\n",
        "    torch.save(model.state_dict() if hasattr(model, 'state_dict') else model,\n",
        "               os.path.join(backup_dir, \"model_backup.bin\"))\n",
        "\n",
        "print(f\"Backup saved to: {backup_dir}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KigBiUclnk5-",
        "outputId": "4c95a826-9df4-4fdf-8bce-566a4ca38e99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving to: ./outputs/final_model_20251206_214543\n",
            "Saved model state_dict\n",
            "Saved best model state\n",
            "✓ Saved tokenizer\n",
            "✓ Saved training history\n",
            "✓ Saved training config\n",
            "✓ Saved metrics\n",
            "✓ Copied checkpoint file\n",
            "All files saved to: ./outputs/final_model_20251206_214543\n",
            "\n",
            "Saved files:\n",
            "  - added_tokens.json\n",
            "  - best_model_state.bin\n",
            "  - checkpoint.pt\n",
            "  - merges.txt\n",
            "  - metrics.json\n",
            "  - pytorch_model.bin\n",
            "  - recovery_script.py\n",
            "  - special_tokens_map.json\n",
            "  - tokenizer.json\n",
            "  - tokenizer_config.json\n",
            "  - training_config.json\n",
            "  - training_history.json\n",
            "  - vocab.json\n",
            "\n",
            "Creating backup of model weights...\n",
            "Backup saved to: ./outputs/backup_20251206_214543\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SAVE FINAL MODEL\n",
        "\n",
        "save_dir = f\"{config.OUTPUT_DIR}/final_model\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Check what type model is\n",
        "print(f\"Model type: {type(model)}\")\n",
        "\n",
        "# If model is already a state_dict (OrderedDict)\n",
        "if isinstance(model, dict):\n",
        "    # Save it directly\n",
        "    torch.save(model, os.path.join(save_dir, \"pytorch_model.bin\"))\n",
        "    print(f\"✓ Model weights saved (was already state_dict)\")\n",
        "else:\n",
        "    # If it's a model object\n",
        "    torch.save(model.state_dict(), os.path.join(save_dir, \"pytorch_model.bin\"))\n",
        "    print(f\"✓ Model weights saved\")\n",
        "\n",
        "# Save tokenizer (this should work)\n",
        "tokenizer.save_pretrained(save_dir)\n",
        "print(f\"✓ Tokenizer saved\")\n",
        "\n",
        "print(f\"\\n✓ All files saved to: {save_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvV8HQoZm55u",
        "outputId": "1843a9f9-ecbc-48c4-ced6-ca73ae7e9d50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model type: <class 'collections.OrderedDict'>\n",
            "✓ Model weights saved (was already state_dict)\n",
            "✓ Tokenizer saved\n",
            "\n",
            "✓ All files saved to: ./outputs/final_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'scheduler_state_dict': scheduler.state_dict(),\n",
        "    'config': config,\n",
        "}, f\"{config.OUTPUT_DIR}/model_checkpoint.pt\")\n",
        "\n",
        "print(\"Checkpoint saved as model_checkpoint.pt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sn4CB2mFA85",
        "outputId": "9127bb80-6579-4709-8a8b-1a6fbb3def30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved as model_checkpoint.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQohh3gYpWGX",
        "outputId": "40fc694e-21bb-45c0-eabd-eb877754ea99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.10.7-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib)\n",
            "  Downloading contourpy-1.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib)\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib)\n",
            "  Downloading fonttools-4.61.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (113 kB)\n",
            "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
            "  Downloading kiwisolver-1.4.9-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: numpy>=1.23 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (2.2.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (11.0.0)\n",
            "Collecting pyparsing>=3 (from matplotlib)\n",
            "  Downloading pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Downloading matplotlib-3.10.7-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contourpy-1.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (355 kB)\n",
            "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading fonttools-4.61.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kiwisolver-1.4.9-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
            "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [matplotlib]\n",
            "\u001b[1A\u001b[2KSuccessfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.61.0 kiwisolver-1.4.9 matplotlib-3.10.7 pyparsing-3.2.5\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seaborn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5nuo2-A-pbj_",
        "outputId": "c8b795f2-fa04-4b14-b014-173902fd2cd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting seaborn\r\n",
            "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\r\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /opt/conda/lib/python3.11/site-packages (from seaborn) (2.2.6)\r\n",
            "Requirement already satisfied: pandas>=1.2 in /opt/conda/lib/python3.11/site-packages (from seaborn) (2.3.3)\r\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /opt/conda/lib/python3.11/site-packages (from seaborn) (3.10.7)\r\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.3)\r\n",
            "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\r\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.61.0)\r\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.9)\r\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\r\n",
            "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.0.0)\r\n",
            "Requirement already satisfied: pyparsing>=3 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.5)\r\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.11/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\r\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2025.2)\r\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2025.2)\r\n",
            "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
            "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
            "Installing collected packages: seaborn\n",
            "Successfully installed seaborn-0.13.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PROBLEM 1.3**"
      ],
      "metadata": {
        "id": "rHapJXsPIeb1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ERROR ANALYSIS**"
      ],
      "metadata": {
        "id": "scaMkE8-HZjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "MODEL_PATH = \"outputs/final_model_complete\"\n",
        "DATA_PATH = \"training.1600000.processed.noemoticon.csv\"\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "xRd8TX4wpRPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For the most recent save\n",
        "MODEL_PATH = \"outputs/final_model_20251206_214543\"\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "\n",
        "# Load model weights\n",
        "model_state = torch.load(f\"{MODEL_PATH}/pytorch_model.bin\", map_location=device)\n",
        "\n",
        "# Create model and load weights\n",
        "model = OptimizedSentimentModel(\n",
        "    model_name=\"roberta-large\",\n",
        "    num_labels=2,\n",
        "    dropout=0.1,\n",
        "    use_multi_sample_dropout=True\n",
        ").to(device)\n",
        "\n",
        "model.transformer.resize_token_embeddings(len(tokenizer))\n",
        "model.load_state_dict(model_state)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygbFJi_MqChP",
        "outputId": "c7c05c87-51f5-40a6-f697-35af9dddeb4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OptimizedSentimentModel(\n",
              "  (transformer): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50280, 1024, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 1024)\n",
              "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-23): 24 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSdpaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): RobertaPooler(\n",
              "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (attention_pooling): AttentionPooling(\n",
              "    (attention): Sequential(\n",
              "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "      (1): Tanh()\n",
              "      (2): Linear(in_features=1024, out_features=1, bias=False)\n",
              "    )\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "    (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    (2): GELU(approximate='none')\n",
              "    (3): Dropout(p=0.1, inplace=False)\n",
              "    (4): MultiSampleDropout(\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (classifiers): ModuleList(\n",
              "        (0-4): 5 x Linear(in_features=1024, out_features=2, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD MODEL\n",
        "\n",
        "# Load tokenizer\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "\n",
        "print(f\"  Vocab size: {len(tokenizer)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIAuwTUwpsHP",
        "outputId": "b49d2c58-d5aa-44c7-e17a-2678baed997b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Vocab size: 50280\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"test_dataset:\", len(test_dataset))\n",
        "print(\"test_loader batches:\", len(test_loader))\n",
        "print(\"total preds:\", len(all_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obiJbOYZvvit",
        "outputId": "31cc8ea4-ac95-4f2c-988d-896e6c1cd419"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_dataset: 160000\n",
            "test_loader batches: 625\n",
            "total preds: 160000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ze65GifR5mcN",
        "outputId": "82129c90-848d-4a82-cc4f-55c7ecc9543d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OptimizedSentimentModel(\n",
              "  (transformer): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50280, 1024, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 1024)\n",
              "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-23): 24 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSdpaSelfAttention(\n",
              "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): RobertaPooler(\n",
              "      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (attention_pooling): AttentionPooling(\n",
              "    (attention): Sequential(\n",
              "      (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "      (1): Tanh()\n",
              "      (2): Linear(in_features=1024, out_features=1, bias=False)\n",
              "    )\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "    (1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    (2): GELU(approximate='none')\n",
              "    (3): Dropout(p=0.1, inplace=False)\n",
              "    (4): MultiSampleDropout(\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (classifiers): ModuleList(\n",
              "        (0-4): 5 x Linear(in_features=1024, out_features=2, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "from tqdm import tqdm\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader, desc=\"Running inference\"):\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "\n",
        "        if isinstance(outputs, dict):\n",
        "            logits = outputs[\"logits\"]\n",
        "        else:\n",
        "            logits = outputs.logits if hasattr(outputs, \"logits\") else outputs\n",
        "\n",
        "\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "print(\"Labels:\", len(all_labels))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyJIDTkV5qqF",
        "outputId": "c2e5334d-477b-4a4b-f626-b665cdbc834f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Running inference: 100%|██████████| 625/625 [02:25<00:00,  4.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preds: 160000\n",
            "Labels: 160000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "    all_labels, all_preds, average='weighted'\n",
        ")\n",
        "\n",
        "print(\"\\n===== Test Metrics =====\")\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1:\", f1)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ww2K0qPI5qk9",
        "outputId": "ab01b728-46ac-4dc0-e070-3080a38eb8ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Test Metrics =====\n",
            "Accuracy: 0.88145625\n",
            "Precision: 0.8815814326183605\n",
            "Recall: 0.88145625\n",
            "F1: 0.881446526758165\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.89      0.88     80000\n",
            "           1       0.89      0.87      0.88     80000\n",
            "\n",
            "    accuracy                           0.88    160000\n",
            "   macro avg       0.88      0.88      0.88    160000\n",
            "weighted avg       0.88      0.88      0.88    160000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "print(\"Confusion Matrix (raw values):\")\n",
        "print(cm)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=[\"Pred 0\", \"Pred 1\"],\n",
        "            yticklabels=[\"True 0\", \"True 1\"])\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "9UTRPX5V9wK5",
        "outputId": "a7845f07-4f73-4775-85bb-11902882b41a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix (raw values):\n",
            "[[71241  8759]\n",
            " [10208 69792]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 2 Axes>"
            ],
            "image/png": "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"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "all_texts = []\n",
        "all_labels = []\n",
        "all_preds = []\n",
        "all_probs = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader, desc=\"Collecting errors\"):\n",
        "\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs[\"logits\"] if isinstance(outputs, dict) else outputs.logits\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        # Store results\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_probs.extend(probs[:, 1].cpu().numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7o_Y_xa-JF6",
        "outputId": "db6d731f-1b5a-47c0-d6fd-b66cd4ec5b99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Collecting errors: 100%|██████████| 625/625 [02:29<00:00,  4.18it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_texts = [\n",
        "    tokenizer.decode(test_dataset[i][\"input_ids\"], skip_special_tokens=True)\n",
        "    for i in range(len(test_dataset))\n",
        "]"
      ],
      "metadata": {
        "id": "boTZq542A3NV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BUILD MISCLASSIFIED LIST\n",
        "\n",
        "misclassified = []\n",
        "\n",
        "for text, true_label, pred_label, prob in zip(all_texts, all_labels, all_preds, all_probs):\n",
        "    if true_label != pred_label:\n",
        "        misclassified.append({\n",
        "            \"text\": text,\n",
        "            \"true\": int(true_label),\n",
        "            \"pred\": int(pred_label),\n",
        "            \"prob\": float(prob)\n",
        "        })\n",
        "\n",
        "print(\"Total misclassified samples:\", len(misclassified))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Q-Ly54bBEBM",
        "outputId": "7dcf9833-8b4c-4eae-8d3b-9e13a244b8ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total misclassified samples: 18967\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SAMPLE 50 MISCLASSIFIED\n",
        "\n",
        "random.seed(42)\n",
        "sample_errors = random.sample(misclassified, min(500, len(misclassified)))"
      ],
      "metadata": {
        "id": "kuXGSLbtBD-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CATEGORIZATION HEURISTICS\n",
        "\n",
        "def categorize_error(text):\n",
        "    t = text.lower()\n",
        "\n",
        "    # 1. Sarcasm\n",
        "    sarcasm_markers = [\"yeah right\", \"sure\", \"totally\", \"/s\", \"as if\", \"great...\", \"love that for me\"]\n",
        "    if any(s in t for s in sarcasm_markers):\n",
        "        return \"sarcasm\"\n",
        "\n",
        "    # 2. Negation\n",
        "    if \"not \" in t or \"n't \" in t:\n",
        "        return \"negation\"\n",
        "\n",
        "    # 3. OOV / Slang / Emojis\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                               u\"\\U0001F600-\\U0001F64F\"\n",
        "                               u\"\\U0001F300-\\U0001F5FF\"\n",
        "                               u\"\\U0001F680-\\U0001F6FF\"\n",
        "                               u\"\\U0001F1E0-\\U0001F1FF\"\n",
        "                               \"]+\", flags=re.UNICODE)\n",
        "    slang = [\"idk\", \"imo\", \"lmao\", \"omg\", \"fr\", \"ngl\", \"smh\"]\n",
        "    if emoji_pattern.search(text) or any(s in t for s in slang):\n",
        "        return \"oov_slang\"\n",
        "\n",
        "    # 4. Multi-topic\n",
        "    if text.count(\".\") > 1 or (\" and \" in t and len(text.split()) > 20):\n",
        "        return \"multi_topic\"\n",
        "\n",
        "    # 5. Domain drift\n",
        "    domain_terms = [\"crypto\", \"elon\", \"ai\", \"nft\", \"stocks\", \"fed\", \"inflation\"]\n",
        "    if any(d in t for d in domain_terms):\n",
        "        return \"domain_drift\"\n",
        "\n",
        "    return \"other\"\n",
        "\n",
        "\n",
        "# Organize categories\n",
        "categories = {\n",
        "    \"sarcasm\": [],\n",
        "    \"negation\": [],\n",
        "    \"oov_slang\": [],\n",
        "    \"multi_topic\": [],\n",
        "    \"domain_drift\": [],\n",
        "    \"other\": []\n",
        "}\n",
        "\n",
        "for item in sample_errors:\n",
        "    cat = categorize_error(item[\"text\"])\n",
        "    categories[cat].append(item)\n"
      ],
      "metadata": {
        "id": "6ZUE9BIWBD8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PRINT REQUIRED OUTPUT\n",
        "\n",
        "def show_category(cat_name, display_name, explanation, fix):\n",
        "    print(\"\\n==============================================================\")\n",
        "    print(f\"### {display_name.upper()}\")\n",
        "    print(\"Why the model failed:\", explanation)\n",
        "    print(\"How to fix it:\", fix)\n",
        "\n",
        "    examples = categories[cat_name][:2]\n",
        "    for ex in examples:\n",
        "        print(\"\\n--- Tweet Example ---\")\n",
        "        print(ex[\"text\"])\n",
        "        print(\"True Label:\", ex[\"true\"], \" Pred:\", ex[\"pred\"], \" Prob:\", round(ex[\"prob\"], 3))\n",
        "\n",
        "\n",
        "show_category(\n",
        "    \"sarcasm\",\n",
        "    \"Sarcasm\",\n",
        "    \"Sarcasm expresses opposite meaning, confusing literal sentiment models.\",\n",
        "    \"Train on sarcasm datasets, add sarcasm markers, or use context-aware models.\"\n",
        ")\n",
        "\n",
        "show_category(\n",
        "    \"negation\",\n",
        "    \"Negation Errors\",\n",
        "    \"Model picks up sentiment words but ignores negation cues ('not happy').\",\n",
        "    \"Use negation-aware augmentation and dependency parsing.\"\n",
        ")\n",
        "\n",
        "show_category(\n",
        "    \"oov_slang\",\n",
        "    \"OOV / Slang / Emojis\",\n",
        "    \"Tweet uses slang/emojis not in pretrained vocabulary, losing sentiment cues.\",\n",
        "    \"Use BERTweet tokenizer, emoji embeddings, or slang normalization.\"\n",
        ")\n",
        "\n",
        "show_category(\n",
        "    \"multi_topic\",\n",
        "    \"Multi-topic Tweets\",\n",
        "    \"Tweets contain multiple conflicting sentiments; model forces binary decision.\",\n",
        "    \"Use multi-label models or sentence-level sentiment decomposition.\"\n",
        ")\n",
        "\n",
        "show_category(\n",
        "    \"domain_drift\",\n",
        "    \"Domain Drift\",\n",
        "    \"Model was trained on general data but tweet refers to domain-specific contexts.\",\n",
        "    \"Fine-tune on domain-specific corpora or adopt adapter layers.\"\n",
        ")\n",
        "\n",
        "print(\"\\nError analysis complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4lTBGvdBD5m",
        "outputId": "9eb3dcac-96a5-4ad3-9d46-22aeab47fc71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================================================\n",
            "### SARCASM\n",
            "Why the model failed: Sarcasm expresses opposite meaning, confusing literal sentiment models.\n",
            "How to fix it: Train on sarcasm datasets, add sarcasm markers, or use context-aware models.\n",
            "\n",
            "--- Tweet Example ---\n",
            "angels demons is brilliant! though not sure 3weeks after an operation 2hrs in a cinema seat was my most brilliant idea!\n",
            "True Label: 0  Pred: 1  Prob: 0.902\n",
            "\n",
            "--- Tweet Example ---\n",
            " sis we totally were not on twitter simultaneously last night must do better today! or tonight ! \n",
            "True Label: 1  Pred: 0  Prob: 0.177\n",
            "\n",
            "==============================================================\n",
            "### NEGATION ERRORS\n",
            "Why the model failed: Model picks up sentiment words but ignores negation cues ('not happy').\n",
            "How to fix it: Use negation-aware augmentation and dependency parsing.\n",
            "\n",
            "--- Tweet Example ---\n",
            " i did not say it she <shout> did <shout> !  seriously and how do i break it to her that she s my daughter and will not ever <shout> be that tan ? \n",
            "True Label: 1  Pred: 0  Prob: 0.019\n",
            "\n",
            "--- Tweet Example ---\n",
            " fuck ya nice let me know how that goes i am doing the same shit down here just not with google\n",
            "True Label: 0  Pred: 1  Prob: 0.631\n",
            "\n",
            "==============================================================\n",
            "### OOV / SLANG / EMOJIS\n",
            "Why the model failed: Tweet uses slang/emojis not in pretrained vocabulary, losing sentiment cues.\n",
            "How to fix it: Use BERTweet tokenizer, emoji embeddings, or slang normalization.\n",
            "\n",
            "--- Tweet Example ---\n",
            "hungry but excited about world duty free staff sale 2day dd <shout>\n",
            "True Label: 0  Pred: 1  Prob: 0.725\n",
            "\n",
            "--- Tweet Example ---\n",
            "i am back from the party! so tired!\n",
            "True Label: 1  Pred: 0  Prob: 0.485\n",
            "\n",
            "==============================================================\n",
            "### MULTI-TOPIC TWEETS\n",
            "Why the model failed: Tweets contain multiple conflicting sentiments; model forces binary decision.\n",
            "How to fix it: Use multi-label models or sentence-level sentiment decomposition.\n",
            "\n",
            "--- Tweet Example ---\n",
            "hard times behind a guy at stop go paying for 10 of gas w 3 ones and <shout> 7 in change <shout> see him outside pumping gas in his hummer <shout>\n",
            "True Label: 1  Pred: 0  Prob: 0.106\n",
            "\n",
            "--- Tweet Example ---\n",
            "what up twitters ? next day in nba <shout> playoffs and next surprises sixers won in orlando atlanta won against heat great defence by hawks\n",
            "True Label: 0  Pred: 1  Prob: 0.973\n",
            "\n",
            "==============================================================\n",
            "### DOMAIN DRIFT\n",
            "Why the model failed: Model was trained on general data but tweet refers to domain-specific contexts.\n",
            "How to fix it: Fine-tune on domain-specific corpora or adopt adapter layers.\n",
            "\n",
            "--- Tweet Example ---\n",
            "i would forgotten how much i missed pushing daisies\n",
            "True Label: 0  Pred: 1  Prob: 0.881\n",
            "\n",
            "--- Tweet Example ---\n",
            " what the hek is the real meaning behind stalkerbait anyway? there are only a posts about it my number is public anyway\n",
            "True Label: 1  Pred: 0  Prob: 0.181\n",
            "\n",
            "Error analysis complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cHcCS7KOrNJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y-0Sxjw8pRA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aKKd1ROcpQ-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t7oyXzEBpQ7u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}